{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUBFQkkuw5ocTNhhzFb5Lz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zypperman/DBTT_G1_GRP3/blob/main/Usecase_6_0_V1(LLMTextFiller).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Case 6.0: Intermediate Loan Default Risk Prediction (LDRP)\n",
        "\n",
        "- Now that we have a basic model to work with, we will now proceed to explore the different kinds of models that can be used and whether they can provide us with more robust results.  \n",
        "  \n",
        "  We will be using:\n",
        "\n",
        "1. A Random Forest (RF) model  (sci-kit learn implemented as `RandomForestClassifier`)\n",
        "2. A gradient boosted decision tree model, from the XGBoost Library (a variant of the scki-kit implemented model `GradientBoostingClassifier` but produced by Nvidia)\n",
        "3. A K nearest neighbours classifier (sci-kit learn implemented as `KNeighborsClassifier`, where we will use `RandomSearchCV` to find an optimal k)\n",
        "4. A Support Vector Machine Classifier (sci-kit learn implemented as `SVC`)\n",
        "4. A mixture of experts model, using code adapted from [this kaggle demo notebook by a user named \"Newton Baba\"](https://www.kaggle.com/code/newtonbaba12345/mixture-of-experts-moe-explained#Implementation-of-MoE)\n",
        "4. Kolmogorov-Arnold Networks (KANs), with a specific schema for credit default prediction named KACDP for high interpretability, obtained from a paper.However for ease of implementation, we will be referring to [this kaggle demo notebook by a user named \"Rashid rk\"](https://www.kaggle.com/code/rashidrk/exploring-kans-v2-0-14887-lb). The paper indicates that this model will yield significant results, and [can be found here](https://arxiv.org/pdf/2411.17783). The data utilized for this paper is the GMSC (Give Me Some Credit) dataset, consisting of 223k records. However, the variables in question seem very reminiscent of the HMEQ dataset, hence we will be reusing the HMEQ data since we are only featuring how these models can be trained and implemented, as opposed to creating an entire production-grade model.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b4h22skdj1F7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWYHcJqRjuY0"
      },
      "outputs": [],
      "source": []
    }
  ]
}