{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use Case 4.0: Application-tagging for identification\n",
        "\n",
        "### Problem statement\n",
        "  In the dashboard, it can be confusing to distinguish applicants from each other, given that there are only a details present. Thai names are a unique problem since they are typically rather long, and make identifying documents error prone. This is especially useful for instances where the loans are specifically about business loans, and a large volume of documents including a business proposal or an earnings report is involved.\n",
        "\n",
        "----\n",
        "\n",
        "### Proposed solution and workflow\n",
        "\n",
        "We propose to attach 3 tags to each loan application based on a pre-selected pool of tags. This is a Guided Topic modelling and tagging task, and we will thus be using BerTopic for this.\n",
        "\n",
        "----\n",
        "### Impact and risks\n",
        "\n",
        "Doing so will enable the officer to work more efficiently, as they will be able to recall applicants by the semantic information that they dealt with prior, or just have a preview to what to expect from the documentation.\n",
        "\n",
        "----\n",
        "\n",
        "### Possible areas for future improvements to be made:\n",
        "\n",
        "We can fine-tune the BerTOPIC pre-trained model for our task using PEFT (Parameter Efficient Fine-Tuning) techniques like [RoSA](https://arxiv.org/abs/2401.04679) and [GaLore](https://arxiv.org/abs/2403.03507) To obtain better responses. Alternatively, we can also fine-tune other aspects of BERTOPIC like its representation models, given that Bertopic is a very versatile and modular framework."
      ],
      "metadata": {
        "id": "Tuh9tGyEBnYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note: Implementation Theory and Source Code referenced from Ashwin N ([Medium](https://medium.com/@ashwinnaidu1991/multi-document-summarization-with-bart-c06db25df62a) \\| [Kaggle](https://www.kaggle.com/code/ashwinnaidu/textsummarization/notebook))\n",
        "\n",
        "# PS: make sure to connect to a runtime with a GPU."
      ],
      "metadata": {
        "id": "KegHOiHBLIEK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VrCB_DKBeb3",
        "outputId": "d1b51078-6e42-4446-827b-93bd108cb7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.4.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llmware transformers huggingface_hub\n",
        "!pip install bertopic\n",
        "!pip install PyPDF2\n",
        "!pip install umap-learn hdbscan scipy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will preprocess a document that we will be using for tagging. For demonstration purposes, we will only be taking the first 3 pages."
      ],
      "metadata": {
        "id": "hErGoiy4PAKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download document\n",
        "import urllib.request\n",
        "from PyPDF2 import PdfReader, PdfWriter # for capacity building, if we decide to expand on the number of pages at 1 go\n",
        "import os\n",
        "\n",
        "pdf_url = \"https://raw.githubusercontent.com/Zypperman/DBTT_G1_GRP3/main/Data/Bank_of_thailand_Statement_2023_EN.pdf\"\n",
        "filename = \"documents/example.pdf\"\n",
        "os.makedirs('./documents',exist_ok=True)\n",
        "\n",
        "def download_pdf_from_github(url, filename):\n",
        "\n",
        "    filepath = f\"./{filename}\"\n",
        "\n",
        "    # Download the file\n",
        "    print(\"Downloading PDF from GitHub\")\n",
        "    urllib.request.urlretrieve(url, filepath)\n",
        "    print(f\"Download complete! File saved at: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "downloaded_filepath = download_pdf_from_github(pdf_url, filename)\n",
        "\n",
        "def get_pages(input_pdf_path, output_pdf_path):\n",
        "    reader = PdfReader(input_pdf_path)\n",
        "    writer = PdfWriter()\n",
        "\n",
        "    for i in range(min(10, len(reader.pages))):\n",
        "        writer.add_page(reader.pages[i])\n",
        "    with open(output_pdf_path, 'wb') as output_file:\n",
        "        writer.write(output_file)\n",
        "\n",
        "get_pages(filename, \"documents/example_short.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRsPox8mPxum",
        "outputId": "a468969f-de58-4cc8-bc12-f16b355f0748"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PDF from GitHub\n",
            "Download complete! File saved at: ./documents/example.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llmware.library import Library\n",
        "\n",
        "fp = './documents/'\n",
        "os.makedirs(fp,exist_ok=True)\n",
        "lib = Library().create_new_library(\"my_library\")\n",
        "lib.add_files(input_folder_path=fp, chunk_size=400, max_chunk_size=600, smart_chunking=0)\n",
        "\n",
        "# standard call to 'ingest' files into a library (implicitly calls Parser and manages the details)\n",
        "lib.add_files(input_folder_path=fp)\n",
        "\n",
        "# NOTE: The above code is meant for RAG applications and is supposed to facilitate larger scale document ingestion. For now, we are able to simply parse the text content into a single string for demonstration.\n",
        "\n",
        "\n",
        "# Load the PDF file\n",
        "filename = \"documents/example.pdf\"\n",
        "reader = PdfReader(\"./\"+filename)\n",
        "\n",
        "Input_text = []\n",
        "\n",
        "# Extract text from each page\n",
        "for page in reader.pages:\n",
        "    Input_text.append(page.extract_text())\n",
        "print(len(Input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqH9sD5bPHjm",
        "outputId": "a54d011f-841d-46eb-cc31-7e9e03827ae5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llmware.parsers:update:  Duplicate files (skipped): 2\n",
            "INFO:llmware.parsers:update:  Total uploaded: 0\n",
            "INFO:llmware.parsers:update:  Duplicate files (skipped): 2\n",
            "INFO:llmware.parsers:update:  Total uploaded: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we install the summarisation transformer, BERTOPIC:"
      ],
      "metadata": {
        "id": "Z9Id2sUGOzHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# Using 25 hand-selected topics\n",
        "topics_to_model = ['Mortgage Loans (Home Loans)',\n",
        " 'Auto Loans',\n",
        " 'Personal Loans',\n",
        " 'Small Business Loans',\n",
        " 'Home Equity Loans & HELOCs',\n",
        " 'Student Loans',\n",
        " 'Construction Loans',\n",
        " 'Debt Consolidation Loans',\n",
        " 'Payday Loans (Short-Term High-Interest Loans)',\n",
        " 'Commercial Real Estate Loans',\n",
        " 'Credit Score & Credit History',\n",
        " 'Debt-to-Income Ratio (DTI)',\n",
        " 'Employment & Income Stability',\n",
        " 'Loan Purpose & Use of Funds',\n",
        " 'Collateral (for Secured Loans)',\n",
        " 'Down Payment or Equity Available',\n",
        " 'Loan Term Preferences',\n",
        " 'Existing Debt Obligations',\n",
        " 'Bankruptcy/Foreclosure History',\n",
        " 'Documentation Accuracy',\n",
        " 'Interest Rate Type (Fixed vs. Variable)',\n",
        " 'Co-Signer or Guarantor Involvement',\n",
        " 'Regulatory Compliance & Loan Eligibility',\n",
        " 'Borrowerâ€™s Savings & Financial Reserves',\n",
        " 'Loan-to-Value Ratio (LTV)',\n",
        " 'Important in mortgage and auto lending.']\n",
        "\n",
        "seed_topic_list = [[i] for i in topics_to_model]\n",
        "\n",
        "# using a smaller embedding model for optimization\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
        "\n",
        "topic_model = BERTopic(seed_topic_list=seed_topic_list,\n",
        "                        umap_model=UMAP(n_components=10),\n",
        "                        hdbscan_model=hdbscan_model,\n",
        "                        embedding_model=embedding_model,\n",
        "                        nr_topics=25,\n",
        "                        low_memory=True\n",
        "                        )\n",
        "Input_text = [doc for doc in Input_text if len(doc.split()) > 5]\n",
        "\n",
        "topics, probs = topic_model.fit_transform(Input_text)"
      ],
      "metadata": {
        "id": "O-GLvpCnOoPk"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has labelled each page with 1 topic, that we can now view:"
      ],
      "metadata": {
        "id": "Z-KkfZzUjiEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Topics by page:\")\n",
        "print(topics)\n",
        "print(\"=\"*40)\n",
        "print(\"Topics by page:\")\n",
        "\n",
        "topic_unique = []\n",
        "for i in topics:\n",
        "    if i not in topic_unique:\n",
        "        topic_unique.append(i)\n",
        "\n",
        "for i in topic_unique:\n",
        "    print(topics_to_model[i]) if i != -1 else 'Bertopic was unsure' # -1 is how bertopic indicates that it isn't sure what topic to assign a page.\n",
        "print(\"=\"*40)\n",
        "\n",
        "print(\"Probabilities (by counting frequency of page labels):\")\n",
        "print(probs)\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(\"Probabilities (of each topic label per page):\")\n",
        "from collections import defaultdict\n",
        "\n",
        "Topics_count = defaultdict(int)\n",
        "\n",
        "for i in topics:\n",
        "    Topics_count[i] += 1\n",
        "\n",
        "Top_probs = {'Bertopic was unsure' if i == -1 else topics_to_model[i]: float(j/37*100) for i, j in Topics_count.items()}\n",
        "\n",
        "for i,j in Top_probs.items():\n",
        "    print('tag:',i,'\\n',f\"probability: {round(j,2)}%\",'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2oPKVtBecwM",
        "outputId": "7e1caf70-7e27-4daf-a73a-0da0196c59ab"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics by page:\n",
            "[1, 1, 1, 1, -1, 2, 1, 1, 1, 2, 2, -1, 1, -1, -1, 2, -1, -1, 2, -1, 2, 2, 2, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, 2, -1]\n",
            "========================================\n",
            "Topics by page:\n",
            "Auto Loans\n",
            "Personal Loans\n",
            "Mortgage Loans (Home Loans)\n",
            "========================================\n",
            "Probabilities (by counting frequency of page labels):\n",
            "[1.         1.         1.         1.         0.         1.\n",
            " 0.98763085 1.         0.94671824 1.         1.         0.\n",
            " 0.95501062 0.         0.         0.92570402 0.         0.\n",
            " 0.91793763 0.         1.         1.         0.93854553 0.\n",
            " 0.         0.         0.         1.         1.         1.\n",
            " 0.         1.         1.         0.         0.         0.95472905\n",
            " 0.        ]\n",
            "========================================\n",
            "Probabilities (of each topic label per page):\n",
            "tag: Auto Loans \n",
            " probability: 21.62% \n",
            "\n",
            "tag: Bertopic was unsure \n",
            " probability: 40.54% \n",
            "\n",
            "tag: Personal Loans \n",
            " probability: 24.32% \n",
            "\n",
            "tag: Mortgage Loans (Home Loans) \n",
            " probability: 13.51% \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Fine-tuning can be performed on the various representation models and embedding models used, but for demonstration purposes we will assume that as the role of MLOPs. For improvements, we can consider performing unsupervised topic modelling to see if any distinct patterns emerge."
      ],
      "metadata": {
        "id": "FRQFFKOTTQDT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06hgj2Szm_DF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
